<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Björn Jörges" />


<title>Estimating JNDs based on staircase procedures – is it valid?</title>

<script src="site_libs/header-attrs-2.19/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">HOME</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="CV.html">CV</a>
</li>
<li>
  <a href="current_projects.html">Current Projects</a>
</li>
<li>
  <a href="finished_projects.html">Published Projects</a>
</li>
<li>
  <a href="papers.html">Papers</a>
</li>
<li>
  <a href="PRISMA.html">PRISMA</a>
</li>
<li>
  <a href="Interesting-Links.html">Interesting Links</a>
</li>
<li>
  <a href="AreJNDsOkay.html">JNDs &amp; Staircase Procedures</a>
</li>
<li>
  <a href="BootstrappedPowerAnalyses.html">Bootstrapped Power Analyses</a>
</li>
<li>
  <a href="JuliaSpeed.html">Fitting GLMMs with Julia</a>
</li>
<li>
  <a href="FixedEffectsRandomEffects.html">Fixed Effects, Random Effects</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Estimating JNDs based on staircase
procedures – is it <em>valid</em>?</h1>
<h4 class="author">Björn Jörges</h4>

</div>


<p>(Spoiler: yeah, it’s probably okay.)</p>
<p>Staircase procedures like the PEST staircase are often used to reduce
the number of trials necessary to estimate PSEs in psychophysical
designs. Some authors (citation need, but probably true) have cautioned
against using the data collected using staircase procedures to obtain
JND estimates as well, alleging that concentrating presented stimulus
strengths around the PSE could lead to biased JND estimates. So what we
are going to do in this lil’ blog post is have a look at this claim
using simulations.</p>
<p>The idea is to generate a dataset of psychophysical response data
with - varying underlying (known) JNDs - varying widths of the function
the presented stimulus strengths are chosen from, i.e., clustered more
or less narrowly around the PSE - varying the number of trias per fitted
psychometric function</p>
<p>This is an adaptation of the procedure that I have published a
PrePrint about a while ago (this one gets a <a
href="https://psyarxiv.com/ack8u/">link</a>, not a citation, what are
we, scientists?), so for a lot of the details on how to simulate
psychphysical data I refer you to that. The following function is
basically straight from the PrePrint and simulates one data set with a
given number of participants, who each have different PSEs and JNDs, and
it gives us the option to simulate two different condition which each
can have different PSEs and JNDs on average. It also allows us to
manipulate the widths of the function of the presentated stimulus
strengths (which we are using a Cauchy function for here, although
simply Gaussian functions can also work).</p>
<pre class="r"><code>SimulatePsychometricData = function(nParticipants,
                                    ConditionOfInterest,
                                    StandardValues,
                                    reps,
                                    PSE_Differences,
                                    JND_Differences,
                                    Multiplicator_PSE_Standard,
                                    Multiplicator_SD_Standard,
                                    Type_ResponseFunction,
                                    SD_ResponseFunctions,
                                    Mean_Variability_Between,
                                    SD_Variability_Between){
  
  
  ID = paste0(&quot;S0&quot;,1:nParticipants)
  
  Psychometric = expand.grid(ID=ID, 
                             ConditionOfInterest=ConditionOfInterest, 
                             StandardValues=StandardValues,
                             PSE_Difference = PSE_Differences,
                             JND_Difference = JND_Differences,
                             reps = 1:reps,
                             SD_ResponseFunction = SD_ResponseFunctions)
  
  Psychometric = Psychometric %&gt;%
    group_by(ID) %&gt;%#
    mutate(PSE_Factor_ID = rnorm(1,1,Mean_Variability_Between), #how much variability is in the means of the psychometric functions between subjects?
           SD_Factor_ID = rnorm(1,1,SD_Variability_Between)) #how much variability is in the standard deviations of the psychometric functions between subjects?
  
  Psychometric = Psychometric %&gt;%
    mutate(
      Mean_Standard = StandardValues+StandardValues*Multiplicator_PSE_Standard,
      SD_Standard = StandardValues*Multiplicator_SD_Standard,
      Mean = (Mean_Standard + (ConditionOfInterest==1)*Mean_Standard*PSE_Difference),
      SD = abs(SD_Standard + (ConditionOfInterest==1)*SD_Standard*JND_Difference))
  
  Psychometric = Psychometric %&gt;%
    mutate(
      Mean = Mean*PSE_Factor_ID,
      SD = SD*SD_Factor_ID)
  
  if (Type_ResponseFunction == &quot;normal&quot;){
    
    Psychometric = Psychometric %&gt;%
      mutate(
        staircase_factor = rnorm(length(reps),1,SD_ResponseFunction*(1+ConditionOfInterest*JND_Difference)))
    
  } else if (Type_ResponseFunction == &quot;Cauchy&quot;){
    
    Psychometric = Psychometric %&gt;%
      mutate(
        staircase_factor = rcauchy(length(reps),1,SD_ResponseFunction*(1+ConditionOfInterest*JND_Difference)))
    
  } else{
    
    print(&quot;distribution not valid&quot;)
    
  }
  
  Psychometric = Psychometric %&gt;%
    mutate(Presented_TestStimulusStrength = Mean*staircase_factor,
           Difference = Presented_TestStimulusStrength - StandardValues)
  
  Psychometric = Psychometric %&gt;%
    mutate(
      AnswerProbability = pnorm(Presented_TestStimulusStrength,Mean,SD),
      
      Answer = as.numeric(rbernoulli(length(AnswerProbability),AnswerProbability))
    )
  
  
  Psychometric = Psychometric %&gt;%
    filter(abs(staircase_factor-1) &lt; 0.75) %&gt;%
    group_by(ID,ConditionOfInterest,StandardValues,Difference) %&gt;%
    mutate(Yes = sum(Answer==1),
           Total = length(ConditionOfInterest))
  
  Psychometric
}</code></pre>
<p>We can use this function to generate data sets with known underlying
parameters. We’re gonna simulate a bunch of datasets where we vary the
difference in PSE and the difference in JNDs between a baseline and an
experimental condition, as well as the width of the distribution that we
draw our stimulus strengths from that we present to our participant. We
draw these values from Cauchy functions, and we vary its scaling factor,
that is its <em>width</em> – higher scaling factor = wider. For all the
other values this function needs to successfully simulate datasets we
simply use the default values that I also use in my little preprint,
shouldn’t really affect the results too much.</p>
<pre class="r"><code># RangeRepetitions = c(40,70,100)
# nParticipants = 100
# ConditionOfInterest = c(0,1)
# StandardValues = c(5)
# PSE_Differences = c(0,0.15)
# JND_Differences = c(0,0.2)
# Multiplicator_PSE_Standard = 0
# Multiplicator_SD_Standard = 0.15
# Type_ResponseFunction = &quot;Cauchy&quot;
# SD_ResponseFunctions = c(0.01,0.02,0.03,0.04,0.05,0.1,0.2)
# Mean_Variability_Between = 0.2
# SD_Variability_Between = 0.2
# 
# DataframeAtrevido = data.frame()
# 
# for (reps in RangeRepetitions){
#   for (SD_ResponseFunction in SD_ResponseFunctions){
#     DataframeAtrevido_Temp = SimulatePsychometricData(nParticipants,
#                                      ConditionOfInterest,
#                                      StandardValues,
#                                      reps,
#                                      PSE_Differences,
#                                      JND_Differences,
#                                      Multiplicator_PSE_Standard,
#                                      Multiplicator_SD_Standard,
#                                      Type_ResponseFunction,
#                                      SD_ResponseFunction,
#                                      Mean_Variability_Between,
#                                      SD_Variability_Between)
#     DataframeAtrevido_Temp$nReps = reps
#     DataframeAtrevido = rbind(DataframeAtrevido,DataframeAtrevido_Temp)
#   }
# }</code></pre>
<p>Since the function filters out presented stimulus strengths that are
too extreme, which could lead to having fewer data points for the wider
functions, which in turn could influence fitted JNDs, we make sure that
the maximum number of trials for any given width is the same.</p>
<pre class="r"><code># DataframeAtrevido = DataframeAtrevido %&gt;%
#   group_by(ID,StandardValues,ConditionOfInterest,SD_ResponseFunction,PSE_Difference,JND_Difference, nReps) %&gt;%
#   mutate(NumberOfTrials = length(ID),
#          RowNumber = 1:length(ID)) %&gt;%
#   group_by(ID,reps,ConditionOfInterest,PSE_Difference,JND_Difference) %&gt;%
#   mutate(MinNumberOfTrials = min(NumberOfTrials)) %&gt;%
#   filter(RowNumber &lt;= MinNumberOfTrials)
# 
# save(DataframeAtrevido, file = paste0(dirname(rstudioapi::getSourceEditorContext()$path),&quot;/AreJNDsOK/DataframeAtrevido.RData&quot;))
load(file = &quot;DataframeAtrevido.RData&quot;)</code></pre>
<pre class="r"><code># JNDs = quickpsy::quickpsy(DataframeAtrevido,
#                           Presented_TestStimulusStrength,
#                           Yes,
#                           grouping = .(ID,StandardValues,ConditionOfInterest,nReps,SD_ResponseFunction,PSE_Difference,JND_Difference),
#                           bootstrap = &quot;none&quot;)
# 
# FittedJNDs = JNDs$par %&gt;%
#     filter(parn == &quot;p2&quot;) %&gt;%
#     mutate(ActualSD = SD)
# save(FittedJNDs, file = paste0(dirname(rstudioapi::getSourceEditorContext()$path),&quot;/AreJNDsOK/FittedJNDs.RData&quot;))
load(file = &quot;FittedJNDs.RData&quot;)</code></pre>
<p>Please note that, given that we simulate 100 participants, fitting
the psychometric functions can take a while, so I just did that once,
saved the data set and load it when needed. Same for the response data
set “DataframeAtrevido”.</p>
<div id="results" class="section level2">
<h2>Results</h2>
<p>Now for the interesting (to some, hopefully) part. Let’s first have a
look at how the presented stimulus strengths are distributed around the
PSEs for different scale factors of the Cauchy function. I chose the
100-trials-per-JND condition for this illustration.</p>
<pre class="r"><code>ggplot(DataframeAtrevido %&gt;% filter(ID == &quot;S01&quot; &amp; ConditionOfInterest == 1 &amp; JND_Difference == 0 &amp; PSE_Difference == 0 &amp; nReps == 100) %&gt;% 
         mutate(FunctionType = paste0(&quot;Cauchy (&quot;,SD_ResponseFunction,&quot;)&quot;)),
       aes(Presented_TestStimulusStrength)) +
  geom_density() +
  facet_wrap(FunctionType~.) +
  xlab(&quot;Presented Stimulus Strength&quot;) +
  ylab(&quot;Density&quot;)</code></pre>
<p><img src="AreJNDsOkay_files/figure-html/plot0-1.png" width="672" /></p>
<p>And this is what the corresponding psychometric functions might look
like:</p>
<pre class="r"><code>ggplot(DataframeAtrevido %&gt;% filter(ID == &quot;S01&quot; &amp; ConditionOfInterest == 1 &amp; JND_Difference == 0 &amp; PSE_Difference == 0 &amp; nReps == 100),
       aes(Presented_TestStimulusStrength, Answer)) +
  binomial_smooth() +
  geom_point() +
  facet_wrap(SD_ResponseFunction~.) +
  xlab(&quot;Presented Stimulus Strength&quot;) +
  ylab(&quot;Probability to answer that PEST is stronger&quot;)</code></pre>
<p><img src="AreJNDsOkay_files/figure-html/plot0.5-1.png" width="672" /></p>
<p>Now let’s look at the fitted versus real JNDs – here’s the plot where
we have no difference between the baseline condition and the
experimental condition (nor in PSEs nor in JNDs). On the y axis we got
the fitted JNDs minus the actual, underlying JNDs. The different panels
are for different scales of the Cauchy function used to present the
stimulus – from “everything very much centered around the PSE” to “a
wider spread, a bit more similar to a Method of Constant Stimuli
situation”.</p>
<pre class="r"><code>ggplot(FittedJNDs %&gt;% filter(JND_Difference == 0 &amp; PSE_Difference == 0 &amp; nReps == 100),aes(as.factor(ConditionOfInterest),par-ActualSD)) +
  geom_jitter(alpha = 0.2, width = 0.1) +
  geom_flat_violin(size=1) +
  ylim(c(-1,4)) +
  xlab(&quot;Experimental Condition&quot;) +
  ylab(&quot;Fitted JND - Actual JND&quot;) +
  stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, size = 6, aes(group=ID), shape = 95) +
  stat_summary(fun = &quot;mean&quot;, geom = &quot;point&quot;, size = 8, aes(group=0), shape = 16, color = &quot;black&quot;) +
  facet_wrap(SD_ResponseFunction~.) +
  ggtitle(&quot;No effect of Experimental Condition on PSE, no effect of Experimental Condition on JND&quot;) +
  geom_hline(yintercept = 0, linetype = 2)</code></pre>
<p><img src="AreJNDsOkay_files/figure-html/plot1-1.png" width="864" /></p>
<p>It’s all a bit hard to see in this type of plot, so let’s boil it
down to the mean difference between the fitted JNDs and the actual,
underlying JNDs:</p>
<pre class="r"><code>FittedJNDs_Means_Diffs = FittedJNDs %&gt;%
  filter(par &lt; 5) %&gt;% 
  group_by(SD_ResponseFunction, ConditionOfInterest,PSE_Difference,JND_Difference) %&gt;%
  mutate(Mean_JND_Difference = mean(par-ActualSD)) %&gt;%
  slice(1) %&gt;% 
  mutate(FunctionType = paste0(&quot;Cauchy\n(&quot;,SD_ResponseFunction,&quot;)&quot;),
         PSE_Dif = paste0(&quot;PSE Difference: &quot;, PSE_Difference),
         JND_Dif = paste0(&quot;JND Difference: &quot;, JND_Difference)) %&gt;%
  ungroup() %&gt;% 
  dplyr::select(FunctionType,ConditionOfInterest,Mean_JND_Difference,PSE_Dif,JND_Dif)</code></pre>
<div id="so-is-the-width-of-this-cauchy-weirdo-function-an-issue"
class="section level4">
<h4>So is the width of this Cauchy weirdo function an issue?</h4>
<p>Here we got the mean difference between the fitted JNDs and the
actual JNDs for each of the differences in JNDs and PSEs between
baseline and experimental condition, further divided up by the width of
the Cauchy distribution the presented stimulus strengths were chosen
from. We also have the number of trials each staircase in based on as
columns of panels. The dashed line at y = 0 indicates no differences
between fitted and actual JNDs.</p>
<pre class="r"><code>ggplot(FittedJNDs_Means_Diffs,aes(FunctionType,Mean_JND_Difference,color = as.factor(ConditionOfInterest))) +
  geom_point(size = 10) +
  geom_line() +
  xlab(&quot;Response Function&quot;) +
  ylab(&quot;Mean Fitted JND - Actual JND&quot;) +
  scale_color_manual(name = &quot;Experimental\nCondition&quot;,values = c(&quot;orange&quot;,&quot;blue&quot;)) +
  facet_grid(PSE_Dif~JND_Dif) +
  scale_x_discrete(labels = c(0.01,0.02,0.03,0.04,0.05,0.1,0.2), name = &quot;Scale Factor of Cauchy Function&quot;) + 
  geom_hline(yintercept = 0, linetype = 2, size = 2)</code></pre>
<p><img src="AreJNDsOkay_files/figure-html/plot5-1.png" width="960" /></p>
<p>So we can see that the JNDs are overestimated when the Cauchy
function is very narrow, and then mostly accurately for the wider
distributions – across the board. That’s somewhat concerning because it
would be nice if the fitted JNDs corresponded to the actual JNDs … but
alas, we can’t have nice things. <em>However, what’s more important
though is that this discrepancy between fitted and real JNDs does not
appear to vary significantly with simulated differences between baseline
and experimental condition. Cool!</em></p>
</div>
<div
id="uh-okay-sure-but-what-about-the-number-of-trials-per-staircase-anything-going-on-there"
class="section level4">
<h4>Uh okay sure but what about the number of trials per staircase,
anything going on there?</h4>
<p>Now what about the number of trials per JND? Here’s the same data,
but broken down into JNDs that are based on 40, 70 and 100 trials.</p>
<pre class="r"><code>FittedJNDs_Means_nReps = FittedJNDs %&gt;%
  filter(par &lt; 5) %&gt;% 
  group_by(SD_ResponseFunction, ConditionOfInterest,PSE_Difference,JND_Difference,nReps) %&gt;%
  mutate(Mean_JND_Difference = mean(par-ActualSD)) %&gt;%
  slice(1) %&gt;% 
  mutate(FunctionType = paste0(&quot;Cauchy\n(&quot;,SD_ResponseFunction,&quot;)&quot;),
         PSE_Dif = paste0(&quot;PSE Difference: &quot;, PSE_Difference),
         JND_Dif = paste0(&quot;JND Difference: &quot;, JND_Difference)) %&gt;%
  ungroup() %&gt;% 
  dplyr::select(FunctionType,ConditionOfInterest,Mean_JND_Difference,PSE_Dif,JND_Dif,nReps)

ggplot(FittedJNDs_Means_nReps,aes(FunctionType,Mean_JND_Difference,color = as.factor(ConditionOfInterest))) +
  geom_point(size = 10) +
  xlab(&quot;Response Function&quot;) +
  ylab(&quot;Mean Fitted JND - Actual JND&quot;) +
  scale_color_manual(name = &quot;Experimental\nCondition&quot;,values = c(&quot;orange&quot;,&quot;blue&quot;)) +
  facet_grid(PSE_Dif*JND_Dif~nReps) +
  scale_x_discrete(labels = c(0.01,0.02,0.03,0.04,0.05,0.1,0.2), name = &quot;Scale Factor of Cauchy Function&quot;) +
  geom_hline(yintercept = 0, linetype = 2, size = 2)</code></pre>
<p><img src="AreJNDsOkay_files/figure-html/plot6-1.png" width="1152" /></p>
<p>Again nothing’s perfect, but no concerningly consistent trends.</p>
</div>
<div id="other-concerns" class="section level4">
<h4>Other concerns?</h4>
<p>I suppose there are some other concerns like a huge amount of noise
if the staircases are too smoll. JNDs that are based on ~50 trials (a
number of trials that’s often considered enough to assess PSEs) are
gonna be pretty unreliable by themselves. HOWEVER… depending on our
purpose that’s not necessarily a problem. Since this added noise is,
well, random noise, we can make up for this lack of reliability of
individual JND measures by simply collecting data from more
participants.</p>
</div>
</div>
<div id="anyway-whats-the-tldr-again" class="section level2">
<h2>Anyway, what’s the TLDR again?</h2>
<p>It’s probably okay to use JNDs fitted based on staircase data. JNDs
might be overestimated across the board, but if we compare between
conditions that probably cancels out and we’re good. Also the number of
trials per staircase is unlikely to shift fitted JNDs in a lawful
manner.</p>
</div>

<html>
<footer role="contentinfo" id="site-footer">
<small><p class="copyright">&#169; 2019,
		This webpage was created using <a href="http://www.r-project.org">R</a> with <a href="http://rmarkdown.rstudio.com">RMarkdown</a> and hosted on <a href="http://github.com">Github</a> based on <a href="https://github.com/jules32/rmarkdown-website-tutorial"> jules32/rmarkdown-website-tutorial</a> project. </small></p>
</footer>
</html>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
